{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAPAtPsazzui"
      },
      "source": [
        "# Download repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh_eJGhmxav4",
        "outputId": "caba46e7-80c8-4f30-aa93-ed0f2f29b889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CWD: /content/hifi-gan\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess, sys\n",
        "\n",
        "REPO_URL = \"https://github.com/aspisov/hifi-gan.git\"\n",
        "PROJECT_DIR = \"/content/hifi-gan\"\n",
        "USE_COMET = False\n",
        "\n",
        "if USE_COMET:\n",
        "    os.environ[\"COMET_API_KEY\"] = \"\"\n",
        "if not os.path.exists(PROJECT_DIR):\n",
        "    subprocess.check_call([\"git\", \"clone\", REPO_URL, PROJECT_DIR])\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(\"CWD:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOnuNyFcz27e"
      },
      "source": [
        "## Download model and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGoO4UiayF32",
        "outputId": "1c186443-6d00-4aba-b90a-03642299504d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m139 packages\u001b[0m \u001b[2min 905ms\u001b[0m\u001b[0m\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=14YIifRMW8TJzYgoolAZYssF2P0r0KB0k\n",
            "From (redirected): https://drive.google.com/uc?id=14YIifRMW8TJzYgoolAZYssF2P0r0KB0k&confirm=t&uuid=ee2ddc75-ff98-43dd-9934-ae250a96b25e\n",
            "To: /content/hifi-gan/checkpoint-epoch100-50000.pth\n",
            "100% 514M/514M [00:10<00:00, 48.1MB/s]\n",
            "Download complete.\n",
            "Extracting ./test_data.zip...\n",
            "Extraction complete!\n"
          ]
        }
      ],
      "source": [
        "!bash scripts/download_model.sh\n",
        "!bash scripts/download_dataset.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LajDSK3tzc65"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"MPLBACKEND\"] = \"Agg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6NjryJbz6z6"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ9XgCLkyhZ8",
        "outputId": "72046298-ec43-4e43-dbb1-10d697587002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading the checkpoint for WV-MOS\n",
            "Weights downloaded in: /root/.cache/wv_mos/wv_mos.ckpt Size: 378728387\n",
            "/content/hifi-gan/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using '/content/hifi-gan/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m COMET_API_KEY was set, but is ignored in offline experiment; remember to set when you upload\n",
            "Loading model weights from: checkpoint-epoch100-50000.pth ...\n",
            "test:   0% 0/5 [00:00<?, ?it/s]W1218 12:10:13.691000 1011 .venv/lib/python3.12/site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "test: 100% 5/5 [00:47<00:00,  9.43s/it]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml OfflineExperiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : testing\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : [OfflineExperiment will get URL after upload]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name               : testing\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams    : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     offline_experiment : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|_target_                                                      : torch.utils.data.DataLoader\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|batch_size                                                    : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|num_workers                                                   : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|pin_memory                                                    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|_target_                                                   : src.datasets.CustomDirAudioDataset\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|audio_dir                                                  : test_data/gt_audio\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|instance_transforms                                        : ${transforms.instance_transforms.inference}\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|missing_text_fallback                                      : \n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|segment                                                    : inference\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|transcription_dir                                          : test_data/transcriptions\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     generator|_target_                                                       : src.model.HiFiGanGenerator\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|device                                                        : auto\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|device_tensors                                                : ['spectrogram', 'audio']\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|from_pretrained                                               : checkpoint-epoch100-50000.pth\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|save_path                                                     : external_resyn\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|seed                                                          : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics|inference                                                        : []\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics|train                                                            : []\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|batch_transforms|inference                                    : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|batch_transforms|train                                        : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|inference|get_spectrogram|_target_        : src.transforms.MelSpectrogram\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|inference|get_spectrogram|config|_target_ : src.transforms.MelSpectrogramConfig\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|inference|get_spectrogram|normalize_audio : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|train|get_spectrogram|_target_            : src.transforms.MelSpectrogram\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|train|get_spectrogram|config|_target_     : src.transforms.MelSpectrogramConfig\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|train|get_spectrogram|normalize_audio     : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|_target_                                                          : src.logger.CometMLWriter\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|id_length                                                         : 32\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|log_checkpoints                                                   : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|loss_names                                                        : ['g_loss', 'g_adv_loss', 'g_feat_loss', 'g_mel_loss', 'd_loss', 'd_mpd_loss', 'd_msd_loss']\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|mode                                                              : offline\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|project_name                                                      : HiFi-GAN\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|run_name                                                          : testing\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|workspace                                                         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     audio               : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Begin archiving the offline data.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m To upload this offline experiment, run:\n",
            "    comet upload /content/hifi-gan/.cometml-runs/981e3516c0fc45af9c57d05feaeb686e.zip\n"
          ]
        }
      ],
      "source": [
        "!uv run python synthesize.py -cn=inference_audio \\\n",
        "  datasets=audio \\\n",
        "  ++datasets.test.audio_dir=test_data/gt_audio \\\n",
        "  ++datasets.test.transcription_dir=test_data/transcriptions \\\n",
        "  inferencer.from_pretrained=checkpoint-epoch100-50000.pth \\\n",
        "  inferencer.save_path=external_resyn \\\n",
        "  writer.mode=offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0xGDYYjbWJW",
        "outputId": "94bb2f4a-fc84-4861-85a8-d0a7dd4ca530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-12-18 12:43:21,807][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Using symlink found at '/content/hifi-gan/tts-fastspeech2-ljspeech/hyperparams.yaml'\n",
            "[2025-12-18 12:43:21,808][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/tts-fastspeech2-ljspeech' if not cached\n",
            "[2025-12-18 12:43:22,766][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint save hook for save\n",
            "[2025-12-18 12:43:22,766][speechbrain.utils.checkpoints][DEBUG] - Registered checkpoint load hook for load_if_possible\n",
            "[2025-12-18 12:43:22,767][speechbrain.utils.parameter_transfer][DEBUG] - Collecting files (or symlinks) for pretraining in tts-fastspeech2-ljspeech.\n",
            "[2025-12-18 12:43:22,767][speechbrain.utils.fetching][INFO] - Fetch spn_predictor.ckpt: Using symlink found at '/content/hifi-gan/tts-fastspeech2-ljspeech/spn_predictor.ckpt'\n",
            "[2025-12-18 12:43:22,767][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths[\"spn_predictor\"] = /content/hifi-gan/tts-fastspeech2-ljspeech/spn_predictor.ckpt\n",
            "[2025-12-18 12:43:22,767][speechbrain.utils.fetching][INFO] - Fetch model.ckpt: Using symlink found at '/content/hifi-gan/tts-fastspeech2-ljspeech/model.ckpt'\n",
            "[2025-12-18 12:43:22,768][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths[\"model\"] = /content/hifi-gan/tts-fastspeech2-ljspeech/model.ckpt\n",
            "[2025-12-18 12:43:22,768][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: spn_predictor, model\n",
            "[2025-12-18 12:43:22,768][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): spn_predictor -> /content/hifi-gan/tts-fastspeech2-ljspeech/spn_predictor.ckpt\n",
            "[2025-12-18 12:43:22,768][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): model -> /content/hifi-gan/tts-fastspeech2-ljspeech/model.ckpt\n",
            "[2025-12-18 12:43:22,964][speechbrain.utils.checkpoints][INFO] - Due to replacement compatibility rule '.convs_intermedite'->'.convs_intermediate', renamed `state_dict['postnet.convs_intermedite.0.conv.weight']`->`state_dict['postnet.convs_intermediate.0.conv.weight']`\n",
            "[2025-12-18 12:43:22,964][speechbrain.utils.checkpoints][INFO] - Due to replacement compatibility rule '.convs_intermedite'->'.convs_intermediate', renamed `state_dict['postnet.convs_intermedite.0.conv.bias']`->`state_dict['postnet.convs_intermediate.0.conv.bias']`\n",
            "[2025-12-18 12:43:22,964][speechbrain.utils.checkpoints][INFO] - Due to replacement compatibility rule '.convs_intermedite'->'.convs_intermediate', renamed `state_dict['postnet.convs_intermedite.1.conv.weight']`->`state_dict['postnet.convs_intermediate.1.conv.weight']`\n",
            "[2025-12-18 12:43:22,964][speechbrain.utils.checkpoints][INFO] - Due to replacement compatibility rule '.convs_intermedite'->'.convs_intermediate', renamed `state_dict['postnet.convs_intermedite.1.conv.bias']`->`state_dict['postnet.convs_intermediate.1.conv.bias']`\n",
            "[2025-12-18 12:43:22,964][speechbrain.utils.checkpoints][INFO] - Due to replacement compatibility rule '.convs_intermedite'->'.convs_intermediate', renamed `state_dict['postnet.convs_intermedite.2.conv.weight']`->`state_dict['postnet.convs_intermediate.2.conv.weight']`\n",
            "[2025-12-18 12:43:22,964][speechbrain.utils.checkpoints][INFO] - Due to replacement compatibility rule '.convs_intermedite'->'.convs_intermediate', renamed `state_dict['postnet.convs_intermedite.2.conv.bias']`->`state_dict['postnet.convs_intermediate.2.conv.bias']`\n",
            "[2025-12-18 12:43:23,012][speechbrain.utils.fetching][INFO] - Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/soundchoice-g2p' if not cached\n",
            "[2025-12-18 12:43:23,722][speechbrain.utils.fetching][INFO] - Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/soundchoice-g2p' if not cached\n",
            "[2025-12-18 12:43:25,252][speechbrain.utils.parameter_transfer][DEBUG] - Fetching files for pretraining (no collection directory set)\n",
            "[2025-12-18 12:43:25,252][speechbrain.utils.fetching][INFO] - Fetch model.ckpt: Fetching from HuggingFace Hub 'speechbrain/soundchoice-g2p' if not cached\n",
            "[2025-12-18 12:43:25,794][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths[\"model\"] = /root/.cache/huggingface/hub/models--speechbrain--soundchoice-g2p/snapshots/ad0a4a1b19260ed34165edb9d245d0f7604c2c17/model.ckpt\n",
            "[2025-12-18 12:43:25,794][speechbrain.utils.fetching][INFO] - Fetch ctc_lin.ckpt: Fetching from HuggingFace Hub 'speechbrain/soundchoice-g2p' if not cached\n",
            "[2025-12-18 12:43:26,087][speechbrain.utils.parameter_transfer][DEBUG] - Set local path in self.paths[\"ctc_lin\"] = /root/.cache/huggingface/hub/models--speechbrain--soundchoice-g2p/snapshots/ad0a4a1b19260ed34165edb9d245d0f7604c2c17/ctc_lin.ckpt\n",
            "[2025-12-18 12:43:26,087][speechbrain.utils.parameter_transfer][INFO] - Loading pretrained files for: model, ctc_lin\n",
            "[2025-12-18 12:43:26,087][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): model -> /root/.cache/huggingface/hub/models--speechbrain--soundchoice-g2p/snapshots/ad0a4a1b19260ed34165edb9d245d0f7604c2c17/model.ckpt\n",
            "[2025-12-18 12:43:26,087][speechbrain.utils.parameter_transfer][DEBUG] - Redirecting (loading from local path): ctc_lin -> /root/.cache/huggingface/hub/models--speechbrain--soundchoice-g2p/snapshots/ad0a4a1b19260ed34165edb9d245d0f7604c2c17/ctc_lin.ckpt\n",
            "[2025-12-18 12:43:28,015][speechbrain.dataio.encoder][WARNING] - TextEncoder.expect_len was never called: assuming category count of 42 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "/content/hifi-gan/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Using '/content/hifi-gan/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m COMET_API_KEY was set, but is ignored in offline experiment; remember to set when you upload\n",
            "Loading model weights from: checkpoint-epoch100-50000.pth ...\n",
            "test:   0% 0/5 [00:00<?, ?it/s][2025-12-18 12:43:40,469][speechbrain.dataio.encoder][WARNING] - TextEncoder.expect_len was never called: assuming category count of 31 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "[2025-12-18 12:43:46,605][speechbrain.dataio.encoder][WARNING] - TextEncoder.expect_len was never called: assuming category count of 43 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:01<00:00,  1.29s/it]\n",
            "test:  20% 1/5 [00:51<03:27, 51.98s/it]\n",
            "100% 1/1 [00:00<00:00, 21.44it/s]\n",
            "test:  40% 2/5 [01:19<01:52, 37.46s/it]\n",
            "100% 1/1 [00:00<00:00, 20.95it/s]\n",
            "test:  60% 3/5 [01:44<01:03, 31.88s/it]\n",
            "100% 1/1 [00:00<00:00, 17.56it/s]\n",
            "test:  80% 4/5 [02:13<00:30, 30.63s/it]\n",
            "100% 1/1 [00:00<00:00, 14.52it/s]\n",
            "test: 100% 5/5 [02:46<00:00, 33.33s/it]\n",
            "test_WV_MOS: 3.417564296722412\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml OfflineExperiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : testing\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : [OfflineExperiment will get URL after upload]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     WV_MOS_test : 3.417564296722412\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name               : testing\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams    : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     offline_experiment : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|_target_                                                      : torch.utils.data.DataLoader\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|batch_size                                                    : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|num_workers                                                   : 0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataloader|pin_memory                                                    : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|_target_                                                   : src.datasets.TextPromptDataset\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|instance_transforms                                        : ${transforms.instance_transforms.inference}\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|prompts                                                    : ['This is a placeholder sentence.']\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|prompts_file                                               : data/prompts/inference_prompts.txt\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     datasets|test|segment                                                    : inference\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     generator|_target_                                                       : src.model.HiFiGanGenerator\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|device                                                        : auto\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|device_tensors                                                : ['spectrogram']\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|from_pretrained                                               : checkpoint-epoch100-50000.pth\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|gt_audio_dir                                                  : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|save_path                                                     : grade_sentences\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inferencer|seed                                                          : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics|device                                                           : auto\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics|inference                                                        : [{'_target_': 'src.metrics.WV_MOS_Metric', 'name': 'WV_MOS', 'source_sr': 22050, 'target_sr': 16000, 'device': 'auto'}]\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics|train                                                            : []\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|batch_transforms|inference                                    : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|batch_transforms|train                                        : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|inference|get_spectrogram|_target_        : src.transforms.TextMelSpectrogram\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     transforms|instance_transforms|inference|get_spectrogram|config|_target_ : src.transforms.TextMelSpectrogramConfig\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|_target_                                                          : src.logger.CometMLWriter\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|id_length                                                         : 32\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|log_checkpoints                                                   : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|loss_names                                                        : ['g_loss', 'g_adv_loss', 'g_feat_loss', 'g_mel_loss', 'd_loss', 'd_mpd_loss', 'd_msd_loss']\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|mode                                                              : offline\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|project_name                                                      : HiFi-GAN\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|run_name                                                          : testing\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     writer|workspace                                                         : None\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     audio               : 5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Begin archiving the offline data.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m To upload this offline experiment, run:\n",
            "    comet upload /content/hifi-gan/.cometml-runs/c6cc732aa3b94b24a05cdb7be5178dd3.zip\n"
          ]
        }
      ],
      "source": [
        "!uv run python synthesize.py -cn=inference_text \\\n",
        "  metrics=wv_mos \\\n",
        "  ++datasets.test.prompts_file=inference_prompts.txt \\\n",
        "  inferencer.from_pretrained=checkpoint-epoch100-50000.pth \\\n",
        "  inferencer.save_path=grade_sentences \\\n",
        "  writer.mode=offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ve7g-Tme9zb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
